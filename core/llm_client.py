from typing import List, Dict, Any, Optional
import json
from google import genai
from google.genai import types
from pydantic import BaseModel, Field
from core.exceptions import LLMAnalysisError, ConfigurationError
from config import Config

class FrameInfo(BaseModel):
    """è§†é¢‘å¸§ä¿¡æ¯æ¨¡å‹"""
    frame_id: int = Field(..., description="å¸§ID")
    timestamp: float = Field(..., description="æ—¶é—´æˆ³")
    image_path: str = Field(..., description="å›¾ç‰‡è·¯å¾„")
    width: int = Field(..., description="å¸§å®½åº¦")
    height: int = Field(..., description="å¸§é«˜åº¦")

class DetectionRegion(BaseModel):
    """æ£€æµ‹åŒºåŸŸæ¨¡å‹"""
    frame_id: int = Field(..., description="å¸§ID")
    object_type: str = Field(..., description="å¯¹è±¡ç±»å‹")
    bbox: tuple[int, int, int, int] = Field(..., description="è¾¹ç•Œæ¡† (x, y, width, height)")
    confidence: float = Field(..., ge=0.0, le=1.0, description="ç½®ä¿¡åº¦")
    description: str = Field(..., description="æè¿°")
    track_id: Optional[int] = Field(None, description="è·Ÿè¸ªID")

class GeminiClient:
    """Gemini LLMå®¢æˆ·ç«¯å°è£…"""
    
    def __init__(self, api_key: str = None):
        config = Config()
        self.api_key = api_key or config.gemini_api_key
        self.model_name = config.gemini_model
        
        if not self.api_key:
            raise ConfigurationError("GEMINI_API_KEY is required")
        
        # é…ç½®Google Gen AIå®¢æˆ·ç«¯
        self.client = genai.Client(
            api_key=self.api_key,
            http_options=types.HttpOptions(api_version='v1beta')
        )
    
    async def analyze_video_frames(
        self, 
        frames: List[FrameInfo], 
        target_description: str
    ) -> List[DetectionRegion]:
        """åˆ†æè§†é¢‘å¸§ï¼Œè¿”å›æ£€æµ‹ç»“æœ
        
        Args:
            frames: è§†é¢‘å¸§ä¿¡æ¯åˆ—è¡¨
            target_description: ç›®æ ‡æè¿°
            
        Returns:
            æ£€æµ‹åŒºåŸŸåˆ—è¡¨
        """
        try:
            # æ„å»ºåˆ†æprompt
            prompt = self._build_analysis_prompt(target_description, frames)
            
            # å‡†å¤‡å›¾ç‰‡æ•°æ®
            contents = [prompt]
            for frame in frames:
                try:
                    # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„
                    import base64
                    with open(frame.image_path, 'rb') as f:
                        image_data = base64.b64encode(f.read()).decode()
                    contents.append({
                        "inline_data": {
                            "mime_type": "image/jpeg", 
                            "data": image_data
                        }
                    })
                except Exception as e:
                    raise LLMAnalysisError(f"Failed to load frame {frame.frame_id}: {str(e)}")
            
            # è°ƒç”¨Google Gen AI API
            print(f"ğŸ¤– æ­£åœ¨åˆ†æ {len(frames)} å¸§è§†é¢‘å†…å®¹...")
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=contents
            )
            
            # æ‰“å°LLMåŸå§‹å“åº”
            print(f"ğŸ“¤ LLMå“åº”å†…å®¹ï¼š")
            print(response.text[:500] + "..." if len(response.text) > 500 else response.text)
            
            # è§£æå“åº”
            regions = self._parse_detection_response(response.text)
            print(f"âœ… LLMåˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(regions)} ä¸ªç›®æ ‡åŒºåŸŸ")
            return regions
            
        except Exception as e:
            raise LLMAnalysisError(f"Video frame analysis failed: {str(e)}") from e
    
    async def decompose_task(
        self, 
        user_request: str, 
        available_tools: List[Dict],
        video_info: Dict = None
    ) -> List[Dict]:
        """åˆ†è§£ç”¨æˆ·ä»»åŠ¡ä¸ºæ‰§è¡Œæ­¥éª¤
        
        Args:
            user_request: ç”¨æˆ·è¯·æ±‚
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            video_info: è§†é¢‘ä¿¡æ¯
            
        Returns:
            æ‰§è¡Œæ­¥éª¤åˆ—è¡¨
        """
        try:
            prompt = self._build_task_decomposition_prompt(
                user_request, available_tools, video_info
            )
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            return self._parse_task_response(response.text)
            
        except Exception as e:
            raise LLMAnalysisError(f"Task decomposition failed: {str(e)}") from e
    
    def _build_analysis_prompt(self, target_description: str, frames: List[FrameInfo]) -> str:
        """æ„å»ºè§†é¢‘åˆ†æprompt"""
        frame_data = []
        for frame in frames:
            frame_data.append({
                "frame_id": frame.frame_id,
                "timestamp": frame.timestamp,
                "dimensions": f"{frame.width}x{frame.height}"
            })
        
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†åˆ†ææä¾›çš„è§†é¢‘å¸§ï¼Œå‡†ç¡®è¯†åˆ«ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡å¯¹è±¡ã€‚

ä»»åŠ¡è¦æ±‚:
1. è¯†åˆ«ç›®æ ‡: {target_description}
2. è¿”å›ç²¾ç¡®çš„è¾¹ç•Œæ¡†åæ ‡
3. è¯„ä¼°æ£€æµ‹ç½®ä¿¡åº¦
4. æ ‡æ³¨å¯¹è±¡åœ¨ä¸åŒå¸§é—´çš„å˜åŒ–

è¾“å‡ºæ ¼å¼è¦æ±‚:
å¿…é¡»è¿”å›JSONæ ¼å¼ï¼Œç»“æ„å¦‚ä¸‹:
{{
  "detections": [
    {{
      "frame_id": 1,
      "objects": [
        {{
          "type": "ç›®æ ‡ç±»å‹",
          "bbox": [x, y, width, height],
          "confidence": 0.95,
          "description": "å…·ä½“æè¿°"
        }}
      ]
    }}
  ],
  "summary": "æ•´ä½“åˆ†ææ€»ç»“"
}}

è§†é¢‘å¸§ä¿¡æ¯:
{json.dumps(frame_data, indent=2, ensure_ascii=False)}

è¯·å¼€å§‹åˆ†æ:"""
    
    def _build_task_decomposition_prompt(
        self, 
        user_request: str, 
        available_tools: List[Dict],
        video_info: Dict = None
    ) -> str:
        """æ„å»ºä»»åŠ¡åˆ†è§£prompt"""
        video_info_str = json.dumps(video_info, indent=2, ensure_ascii=False) if video_info else "æ— "
        tools_str = json.dumps(available_tools, indent=2, ensure_ascii=False)
        
        return f"""ç”¨æˆ·è¯·æ±‚: {user_request}
è§†é¢‘ä¿¡æ¯: {video_info_str}

è¯·å°†ç”¨æˆ·è¯·æ±‚åˆ†è§£ä¸ºå…·ä½“çš„æ‰§è¡Œæ­¥éª¤ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„å·¥å…·å‡½æ•°ã€‚

å¯ç”¨å·¥å…·:
{tools_str}

è¯·è¿”å›æ‰§è¡Œè®¡åˆ’çš„JSONæ ¼å¼:
{{
  "steps": [
    {{
      "step_id": 1,
      "description": "æ­¥éª¤æè¿°",
      "tool_name": "å·¥å…·å‡½æ•°å",
      "parameters": {{"param1": "value1"}}
    }}
  ]
}}"""
    
    def _parse_detection_response(self, response_text: str) -> List[DetectionRegion]:
        """è§£ææ£€æµ‹å“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise LLMAnalysisError("No valid JSON found in response")
            
            json_str = response_text[start_idx:end_idx]
            data = json.loads(json_str)
            
            regions = []
            for detection in data.get("detections", []):
                frame_id = detection["frame_id"]
                for obj in detection.get("objects", []):
                    region = DetectionRegion(
                        frame_id=frame_id,
                        object_type=obj["type"],
                        bbox=tuple(obj["bbox"]),
                        confidence=obj["confidence"],
                        description=obj["description"]
                    )
                    regions.append(region)
            
            return regions
            
        except json.JSONDecodeError as e:
            raise LLMAnalysisError(f"Failed to parse detection response: {str(e)}")
        except KeyError as e:
            raise LLMAnalysisError(f"Missing required field in response: {str(e)}")
    
    def _parse_task_response(self, response_text: str) -> List[Dict]:
        """è§£æä»»åŠ¡åˆ†è§£å“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise LLMAnalysisError("No valid JSON found in task response")
            
            json_str = response_text[start_idx:end_idx]
            data = json.loads(json_str)
            
            return data.get("steps", [])
            
        except json.JSONDecodeError as e:
            raise LLMAnalysisError(f"Failed to parse task response: {str(e)}")
        except KeyError as e:
            raise LLMAnalysisError(f"Missing required field in task response: {str(e)}")